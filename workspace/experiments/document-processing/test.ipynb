{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Testing della funzionalità dello Stemmer.\n",
    "Gli stemmer più noti sono Porter e Lancaster, entrambi implementati nella libreria nltk.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\"\"\" Fase 1: Tokenizzazione \"\"\"\n",
    "\n",
    "# Definizione della frase di esempio\n",
    "sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\n",
    "\n",
    "# Tokenizzazione della frase in parole (token)\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Conversione di tutti i token in minuscolo\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "\"\"\" Fase 2: Rimozione delle stopwords \"\"\"\n",
    "\n",
    "# Recupero delle stopwords in inglese\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Rimozione delle stopwords dai token\n",
    "non_stopword_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\"\"\" Fase 3a: Lemmatizzazione dei token \"\"\"\n",
    "\n",
    "# Crea un'istanza del lemmatizzatore\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizzazione dei token non stopwords\n",
    "lemmatized_tokens = [wnl.lemmatize(token) for token in non_stopword_tokens]\n",
    "\n",
    "\"\"\" Fase 3b: Stemming dei token \"\"\"\n",
    "\n",
    "# Crea le istanze degli stemmer\n",
    "porter_stemmer = PorterStemmer()  # Porter Stemmer\n",
    "lancaster_stemmer = LancasterStemmer()  # Lancaster Stemmer\n",
    "\n",
    "# Variabile per scegliere lo stemmer da utilizzare\n",
    "use_porter_stemmer = True\n",
    "\n",
    "# Stemming dei token lemmatizzati utilizzando lo Stemmer scelto\n",
    "if use_porter_stemmer:\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in lemmatized_tokens]\n",
    "else:\n",
    "    stemmed_tokens = [lancaster_stemmer.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "# Stampa dei token dopo il processo di Stemming\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At eight o'clock on Thursday morning Arthur didn't feel very good.\n",
      "['at', 'eight', \"o'clock\", 'on', 'thursday', 'morning', 'arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Fase 1: Tokenizzazione \"\"\"\n",
    "    \n",
    "# Definizione della frase di esempio\n",
    "sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\n",
    "\n",
    "# Tokenizzazione della frase in parole (token)\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Conversione di tutti i token in minuscolo\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Stampa dei token\n",
    "print(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do', 'this', 'below', \"isn't\", 'we', 'our', 'their', 'having', 'who', 'him', 'when', 'some', 'just', 'wasn', 'through', 'at', \"mustn't\", 'any', 'once', 'had', 'by', 'few', 'needn', \"you'd\", 'where', 'won', \"needn't\", 'only', 'out', 'himself', 'while', 'hers', 'were', 'does', 't', 'if', 'very', 'ourselves', 'been', 'so', \"didn't\", 'under', \"doesn't\", 'each', 'than', 'ours', \"wouldn't\", 'from', 'how', 'off', 'doesn', 'both', \"mightn't\", 'same', 'it', 'whom', 'are', 'a', 'more', 'nor', \"aren't\", 'all', 'me', 'over', 'now', 'haven', 'further', \"weren't\", 'what', 'll', 'you', 'being', 'up', 'too', 're', 'against', 'most', \"you've\", 'she', 'of', 'into', 'own', 'or', 'with', 'yourselves', 'down', 'itself', 'such', 'theirs', \"shouldn't\", 'until', 'its', 'but', 'her', 'on', 'because', 'ain', 'don', 'aren', 'y', 'the', 'yours', 'in', \"it's\", \"should've\", \"you'll\", \"you're\", 'above', 'mightn', 'those', \"hasn't\", 'there', 'can', 'for', 'he', 'o', 'they', 'i', 'that', 'm', 'an', \"won't\", \"haven't\", 'herself', 'shouldn', 's', 'isn', 'hadn', 'weren', \"wasn't\", 'shan', 'should', 'myself', 'doing', 'then', 'yourself', \"that'll\", 'hasn', 'these', 'about', 'd', 'mustn', 'your', 'why', 'has', 'is', 'them', 'other', 'after', 'be', 'ma', 'couldn', 'wouldn', 'and', 'themselves', 'not', 'between', \"couldn't\", 'didn', 'will', 'to', 'here', \"shan't\", 'have', \"hadn't\", 'as', 'no', \"she's\", 've', 'was', 'during', 'did', 'my', 'am', 'his', \"don't\", 'which', 'before', 'again'}\n",
      "['eight', \"o'clock\", 'thursday', 'morning', 'arthur', \"n't\", 'feel', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Fase 2: Rimozione delle stopwords \"\"\"\n",
    "    \n",
    "# Recupero delle stopwords in inglese\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Rimozione delle stopwords dai token\n",
    "non_stopword_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Stampa dei token senza stopwords\n",
    "print(stop_words)\n",
    "print(non_stopword_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eight', \"o'clock\", 'thursday', 'morning', 'arthur', \"n't\", 'feel', 'good', '.']\n",
      "['eight', \"o'clock\", 'thursday', 'morning', 'arthur', \"n't\", 'feel', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Fase 3a: Lemmatizzazione dei token \"\"\"\n",
    "\n",
    "# Crea un'istanza del lemmatizzatore\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizzazione dei token non stopwords\n",
    "lemmatized_tokens = [wnl.lemmatize(token) for token in non_stopword_tokens]\n",
    "\n",
    "print(non_stopword_tokens)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eight', \"o'clock\", 'thursday', 'morning', 'arthur', \"n't\", 'feel', 'good', '.']\n",
      "['eight', \"o'clock\", 'thursday', 'morn', 'arthur', \"n't\", 'feel', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Fase 3b: Stemming dei token \"\"\"\n",
    "\n",
    "# Crea le istanze degli stemmer\n",
    "porter_stemmer = PorterStemmer()  # Porter Stemmer\n",
    "lancaster_stemmer = LancasterStemmer()  # Lancaster Stemmer\n",
    "\n",
    "# Variabile per scegliere lo stemmer da utilizzare\n",
    "use_porter_stemmer = True\n",
    "\n",
    "# Stemming dei token lemmatizzati utilizzando lo Stemmer scelto\n",
    "if use_porter_stemmer:\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in lemmatized_tokens]\n",
    "else:\n",
    "    stemmed_tokens = [lancaster_stemmer.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "# Stampa dei token dopo il processo di Stemming\n",
    "print(lemmatized_tokens)\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: At eight o'clock on Thursday morning Arthur didn't feel very good.\n",
      "tokens: ['at', 'eight', \"o'clock\", 'on', 'thursday', 'morning', 'arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      "stop_words: {'do', 'this', 'below', \"isn't\", 'we', 'our', 'their', 'having', 'who', 'him', 'when', 'some', 'just', 'wasn', 'through', 'at', \"mustn't\", 'any', 'once', 'had', 'by', 'few', 'needn', \"you'd\", 'where', 'won', \"needn't\", 'only', 'out', 'himself', 'while', 'hers', 'were', 'does', 't', 'if', 'very', 'ourselves', 'been', 'so', \"didn't\", 'under', \"doesn't\", 'each', 'than', 'ours', \"wouldn't\", 'from', 'how', 'off', 'doesn', 'both', \"mightn't\", 'same', 'it', 'whom', 'are', 'a', 'more', 'nor', \"aren't\", 'all', 'me', 'over', 'now', 'haven', 'further', \"weren't\", 'what', 'll', 'you', 'being', 'up', 'too', 're', 'against', 'most', \"you've\", 'she', 'of', 'into', 'own', 'or', 'with', 'yourselves', 'down', 'itself', 'such', 'theirs', \"shouldn't\", 'until', 'its', 'but', 'her', 'on', 'because', 'ain', 'don', 'aren', 'y', 'the', 'yours', 'in', \"it's\", \"should've\", \"you'll\", \"you're\", 'above', 'mightn', 'those', \"hasn't\", 'there', 'can', 'for', 'he', 'o', 'they', 'i', 'that', 'm', 'an', \"won't\", \"haven't\", 'herself', 'shouldn', 's', 'isn', 'hadn', 'weren', \"wasn't\", 'shan', 'should', 'myself', 'doing', 'then', 'yourself', \"that'll\", 'hasn', 'these', 'about', 'd', 'mustn', 'your', 'why', 'has', 'is', 'them', 'other', 'after', 'be', 'ma', 'couldn', 'wouldn', 'and', 'themselves', 'not', 'between', \"couldn't\", 'didn', 'will', 'to', 'here', \"shan't\", 'have', \"hadn't\", 'as', 'no', \"she's\", 've', 'was', 'during', 'did', 'my', 'am', 'his', \"don't\", 'which', 'before', 'again'}\n",
      "non_stopword_tokens: ['eight', \"o'clock\", 'thursday', 'morning', 'arthur', \"n't\", 'feel', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Fase 1: Tokenizzazione \"\"\"\n",
    "    \n",
    "# Definizione della frase di esempio\n",
    "sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\n",
    "\n",
    "# Tokenizzazione della frase in parole (token)\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Conversione di tutti i token in minuscolo\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "print(f\"sentence: {sentence}\")\n",
    "print(f\"tokens: {tokens}\")\n",
    "\n",
    "\"\"\" Fase 2: Rimozione delle stopwords \"\"\"\n",
    "\n",
    "# Recupero delle stopwords in inglese\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"stop_words: {stop_words}\")\n",
    "# Rimozione delle stopwords dai token\n",
    "non_stopword_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Stampa dei token senza le stopwords\n",
    "print(f\"non_stopword_tokens: {non_stopword_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
